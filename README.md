# NN_Assignmnet_4
# 🧠 CS5720 – Home Assignment 4

## University of Central Missouri  
**Course**: Neural Networks and Deep Learning  
**Instructor**: Dr. I-Hua Tsai  
**Student Name**: *Jagarlamudi Tharaka*  
**Semester**: Summer 2025  

---

## 📘 Overview

This repository contains the implementation and demonstration of **Home Assignment 4**, focused on:
- Generative Adversarial Networks (GANs)
- Data Poisoning Attacks on Sentiment Classifiers

All experiments are conducted using **Google Colab**.

---

## 🧪 Tasks
### **Question 1: GAN Architecture**

**Objective**: Explain the architecture of a GAN, including:

* Roles of the **Generator** and **Discriminator**
* Adversarial process of training
* Competitive improvement

**Deliverables**:

* **Diagram**: `gan_architecture.png`

**Notes**:
The diagram illustrates multilayer neural networks with the Generator creating fake MNIST digits and the Discriminator distinguishing real vs. fake samples. Loss feedback loops are shown clearly.

---

### **Question 2: Ethics of AI and Potential Harm**

**Objective**: Analyze ethical implications of AI, focusing on misinformation in generative AI.

**Deliverable (Narration Text)**:

> "This analysis focuses on misinformation in generative AI as a real-world harm. A hypothetical application is the use of GANs to generate fake news articles, where a model trained on mixed authentic and fabricated data produces convincing but false stories about political events, leading to public confusion and manipulation. This can occur if the training data includes unverified sources. Two mitigation strategies from the lecture include implementing robust data validation processes to filter out unreliable inputs before training, ensuring only verified data is used, and deploying real-time fact-checking algorithms to flag and correct misinformation generated by the model, reducing its societal impact."

---

### ✅ Task 3: Basic GAN on MNIST
- **Tool**: TensorFlow 2.x (Keras)
- **Dataset**: MNIST Handwritten Digits
- **Implementation**:
  - Generator and Discriminator network design
  - Adversarial training loop with alternating updates
  - Image generation at Epochs `0`, `50`, and `100`
  - Visualization of Generator vs Discriminator loss

### 🧨 Task 4: Data Poisoning Simulation *(WIP)*
- Simulate a label-flipping attack on sentiment classification
- Visualize accuracy and confusion matrix:
  - Before poisoning
  - After poisoning

---

## 📁 Files in this Repository

| File Name                                 | Description                              |
|------------------------------------------|------------------------------------------|
| `Home_Assignment_4.ipynb` | Main Colab Notebook for Tasks 3 & 4       |
| `README.md`             | This GitHub-style documentation           |
| `image_at_epoch_000.png` *(output)*       | Sample GAN image at start                 |
| `image_at_epoch_050.png` *(output)*       | Sample GAN image after 50 epochs          |
| `image_at_epoch_100.png` *(output)*       | Final GAN result after 100 epochs         |

---

## ▶️ Running on Google Colab

1. Download the `.ipynb` notebook from this repo.
2. Open [Google Colab](https://colab.research.google.com/).
3. Upload and open the notebook.
4. Run all cells sequentially.
5. Output images and loss curves will be auto-generated.

---

## 📊 Sample Output

### ✅ Generated Images

Images are saved and displayed after:
- Epoch 0
- Epoch 50
- Epoch 100

### 📉 Loss Curve

Includes plot comparing Generator and Discriminator losses.

**Question 5: Generative AI Legal and Ethical Considerations**

**Objective**: Investigate legal and ethical concerns of AI-generated content.

**Deliverable (Narration Text)**:

> "Generative AI raises significant legal and ethical concerns. One example is memorizing private data, as seen with GPT-2, where the model inadvertently retained names or personal details from training texts, violating privacy rights. Another issue is generating copyrighted material, like Harry Potter text, where GANs may reproduce protected content, leading to intellectual property disputes. I believe generative AI models should be restricted from certain data during training, such as private datasets or copyrighted works, to prevent these violations. Justification includes protecting individual privacy and respecting intellectual property laws, though this requires careful curation and legal frameworks to balance innovation with ethical standards."

---

###  **Question 6: Bias and Fairness in AI Systems**

**Objective**: Analyze bias in AI, propose detection and mitigation methods, and emphasize fairness importance.

**Deliverable**:

> "This analysis focuses on the false negative rate parity metric from the Aequitas Bias Audit Tool. This metric measures the equality of false negative rates across different groups, ensuring that the proportion of missed positive predictions is similar for all demographics, such as gender or race. It’s important because unequal false negatives can lead to unfair outcomes, like denying loans to qualified individuals from underrepresented groups. A model might fail this metric if its training data overrepresents one group, causing higher false negatives for others. Mitigation involves reweighting the dataset to balance representation or applying fairness constraints during training to align rates across groups."
